\documentclass[../document.tex]{subfiles}
\begin{document}\label{sec:related_work}
	
The NAS parallel benchmarks~\cite{bailey1991parallel} follow a `pencil-and-paper` approach, specifying the computational problem but leaving implementation choices such as language, data structures and algorithms to the user.
The benchmarks include varied kernels and applications which allow a nuanced evaluation of a complete HPC system, however, the unconstrained approach does not readily support direct performance comparison between different hardware accelerators using a single set of codes.

Martineau et al.~\cite{martineau2016performance} collected a suite of benchmarks and three mini-apps to evaluate Clang OpenMP 4.5 support for Nvidia GPUs.
Their focus was on comparison with CUDA; OpenCL was not considered.

The Scalable Heterogeneous Computing benchmark suite (SHOC)~\cite{lopez2015examining}, unlike OpenDwarfs and Rodinia, supports multiple nodes using MPI for distributed parallelism.
SHOC supports multiple programming models including OpenCL, CUDA and OpenACC, with benchmarks ranging from targeted tests of particular low-level hardware features to a handful of application kernels.
Sun et al.~\cite{sun2016} propose Hetero-Mark, a Benchmark Suite for CPU-GPU Collaborative Computing, which has 5 benchmark applications each implemented in HCC -- which compiles to OpenCL, HIP -- for a CUDA and Radeon Open Compute back-end, and a CUDA version.
Meanwhile, Chai by G{\'o}mez-Luna et al.~\cite{gomez2017chai}, offers 15 applications in 7 different implementations with the focus on supporting integrated architectures.
These benchmark suites centre on a comparison between languages and the corresponding support on heterogeneous architectures; whereas our work focuses on benchmarking for device specific performance limitations by examining the problem sizes where these limitations occur -- this is largely ignored by benchmarking suites with fixed problem sizes.

Additionally, our enhanced OpenDwarfs benchmark suite aims to cover a wider range of application patterns by focusing exclusively on OpenCL using higher-level benchmarks.

\citet{barnes2016evaluating} collected a representative set of applications from the current NERSC workload to guide optimization for Knights Landing in the Cori supercomputer.
As it is not always feasible to perform such a detailed performance study of the capabilities of different computational devices for particular applications, the benchmarks described in this paper may give a rough understanding of device performance and limitations.

\end{document}
