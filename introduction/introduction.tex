\documentclass[../document.tex]{subfiles}
\begin{document}\label{sec:introduction}
	
High performance computing (HPC) hardware is becoming increasingly heterogeneous.
A major motivation for this is to reduce energy use; indeed, without significant improvements in energy efficiency, the cost of exascale computing will be prohibitive.
From June 2016 to June 2017, the average energy efficiency of the top 10 of the Green500 supercomputers rose by 2.3x, from 4.8 to 11.1 gigaflops per watt.\cite{feldman_2017}
For many systems, this was made possible by highly energy-efficient Nvidia Tesla P100 GPUs.
In addition to GPUs, future HPC architectures are also likely to include nodes with FPGA, DSP, ASIC and MIC components.
A single node may be heterogeneous, containing multiple different computing devices; moreover, a HPC system may offer nodes of different types.
For example, the Cori system at Lawrence Berkeley National Laboratory comprises 2,388 Cray XC40 nodes with Intel Haswell CPUs, and 9,688 Intel Xeon Phi nodes~\cite{declerck2016cori}.
The Summit supercomputer at Oak Ridge National Laboratory is based on the IBM Power9 CPU, which includes both NVLINK~\cite{morgan_2016}, a high bandwidth interconnect between Nvidia GPUs; and CAPI, an interconnect to support FPGAs and other accelerators.~\cite{morgan_2017}
Promising next generation architectures include Fujitsu's Post-K~\cite{morgan_2016_postk}, and Cray's CS-400, which forms the platform for the Isambard supercomputer~\cite{feldman_2017_isambard}.
Both architectures use ARM cores alongside other conventional accelerators, with several Intel Xeon Phi and Nvidia P100 GPUs per node.

We seek to answer the question: for a given architecture with a variety of computing devices and a given set of computational tasks, which is the best choice of device for each task?
Furthermore, does the best choice of device vary depending on whether the primary concern is time to solution or energy efficiency?
While we do not yet have a satisfactory answers to these questions, our early investigations have yielded performance insights and valuable benchmarks, which we share in this paper.

As we are interested in comparing a diverse set of computing devices for HPC, we focus on the OpenCL programming model.
OpenCL is supported on a wide range of systems including CPU, GPU and FPGA devices.
While it is possible to write application code directly in OpenCL, it may also be used as a base to implement higher-level programming models.
This technique was shown by Mitra et al.,~\cite{mitra2014implementation} where an OpenMP runtime was implemented over an OpenCL framework for Texas Instruments Keystone II DSP architecture.
Having a common back-end in the form of OpenCL allows a direct comparison of identical code across diverse architectures, including FPGAs.

Our starting point is the OpenDwarfs benchmark suite, a set of OpenCL benchmarks for heterogeneous computing platforms.\cite{krommydas2016opendwarfs}
OpenDwarfs characterizes benchmarks according to patterns of computation and communication known as the 13 Berkeley Dwarfs.\cite{asanovic2006landscape}
We extended OpenDwarfs with additional benchmarks taken from the Rodinia suite~\cite{che2009rodinia} (with modifications to improve portability), and with new benchmarks such as the two-dimensional discrete wavelet transform and a replaced version of the Fast Fourier Transform -- since the original version would frequently throw segmentation faults or generate an incorrect solution in the two dimensional setting.

Marjanovi\'{c} et al.~\cite{marjanovic2016hpc} argue that the selection of problem size for HPC benchmarking critically affects which hardware properties are relevant.
We have observed this to be true across a wide range of accelerators, therefore we have enhanced the OpenDwarfs benchmark suite to support running different problem sizes for each benchmark.
In this paper we report preliminary results for our enhanced version of OpenDwarfs on a range of platforms including CPU, GPU and MIC devices.

% for a short paper, we may not need this roadmap - JM
%Section~\ref{sec:extending_the_opendwarfs_benchmark_suite} discusses the changes made to the OpenDwarfs benchmark suite.
%Related work is presented in Section~\ref{sec:related_work}.
%The experimental setup is presented in Section~\ref{sec:experimental_setup}.
%Performance measurements including time and energy is presented in the results Section~\ref{sec:results}.
%Finally, conclusions and future work are presented in Section~\ref{sec:conclusions} and Section~\ref{sec:future_work} respectively.
\end{document}
