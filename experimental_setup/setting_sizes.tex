\documentclass[../document.tex]{subfiles}
\begin{document}\label{ssec:setting_sizes}

%focus of this is the experimental evaluation to justify setting problem sizes
For each application 4 different problem sizes were selected, namely {\bf tiny}, {\bf small},{\bf medium} and {\bf large}.
This was based on various levels of caching performance on the Skylake CPU.
For instance, tiny should just fit within L1 cache, on the Skylake this corresponds to 32KiB of data cache, small should fit within the 256KiB L2 data cache, medium should fit within 8192KiB of the L3 cache, and large must be greater than 8192KiB to avoid caching and operate out of main memory.
This was verified using the {\tt lscpu} Linux tool.

An experiment was conducted to verify the selection of each problem size, in which an examination of which increased problem sizes resulted in a cache spill-over.
The final selected problem sizes should be as close to filling each level as cache as possible.

Caching performance was collected using the PAPI counters.
On the Skylake L1 and L2 data cache miss rates were counted using the {\tt PAPI\_L1\_DCM} and {\tt PAPI\_L2\_DCM} counters.
For L3 miss events, only the total cache counter event ({\tt PAPI\_L3\_TCM}) was available.
The final values presented as miss results are presented as a percentage, and were determined using the number of misses counted divided by the total instructions ({\tt PAPI\_TOT\_INS}).

The methodology to determine the appropriate size parameters is demonstrated on the k-means benchmark.
The k-means benchmark performs a local clustering to represent a collection of objects by the centroids of a cluster.
Each step of the algorithm computes the nearest centroid for each data object, then relocates the centroid to the mean of all objects within the sub-cluster.
Execution terminates when no clusters change size between iterations.
Starting positions for each centroid is determined randomly.
This benchmark previously required the feature space on which to perform clustering to be provided as a previously generated file which was then loaded.
It was then extended to allow setting the feature space as an argument from the command line.
This feature space is then randomly generated at each run, thereby increasingly the randomness of the distribution for each invocation of the program.
This was done to increase true characteristics of cache performance, since repeated runs of clustering on the same feature space (loaded from file) would deterministically generate similar caching performance.
Each value in the feature space is 32-bit floating point precision.
The same number of clusters is fixed, with the minimum and maximum cluster locations being returned both set to 5.
The only parameters changes between run to increase the problem size is the number of points/objects in this feature space, increasing from 5 to 47, and increasing the number of features for each level of the cache size.
In the kernel for k-means there are 3 large one dimensional arrays passed to the device, namely {\bf feature}, {\bf cluster} and {\bf membership}.
{\bf feature} is the array which stores the unclustered feature space, it is of size $P_n \times F_n \times \text{sizeof}\left(\text{float}\right)$, where $ P_n $ is the number of points in the subspace, $F_n$ is the number of features per point and $\text{sizeof}\left(\text{float}\right)$ is the number of bytes required to represent a floating point value per feature.
{\bf cluster} is the working and output array to store the intermediately clustered points, it is of size $C_n \times F_n \times \text{sizeof}\left(\text{float}\right)$, where $C_n$ is the number of cluster to locate in the subspace.
{\bf membership} is an array indicating whether each point has changed to a new cluster in each iteration of the algorithm, it is of size $P_n \times \text{sizeof}\left(\text{int}\right)$, where $\text{sizeof}\left(\text{int}\right)$ is the number of bytes to represent an integer value.
Thereby the working kernel memory, in KiB, is determined to be:
\begin{equation}
    \frac{\text{size}\left(\textbf{feature}\right)+\text{size}\left(\textbf{membership}\right)+\text{size}\left(\textbf{cluster}\right)}{1024}
    \label{eq:kmeans_size}
\end{equation}

Since it is known for the target Skylake CPU that the L1 cache is of size 32KiB, and using the theoretical size of all memory buffers determined in Equation~\ref{eq:kmeans_size}, a careful experiment can be conducted around each benchmark and the appropriate amount of memory required.
If the feature size were in increments of 1KiB (256 floating point values) and to fit within L1 cache, the optimal number of points/objects in the feature space is 26 (which is just under 32KiB).

Therefore the experiment was designed to increase the size of each increment by increasing the number of features per point/object, then as the number of features is increased the total execution time and respective caching events were measured.

Similarly {\bf small} increases by 2048, or increments of 8KiB per point/object, {\bf medium} increases by 65536 as increments of 256KiB per point/object and {\bf large} increases by 524288 or increments of 2048KiB per object.
Being constrained in such a way indicates that 26 points/objects should give the best L1, L2 and L3 in the {\bf tiny}, {\bf small} and {\bf medium} problem sizes respectively.
Spilling over at each level of cache should be at 27 points/objects for each problem size.
{\bf large} operates solely on main memory.

To measure increase the number of points/objects by 1 from 5 to 47.
Configuration of the experiment is repeated 300 times, and each iteration of the kernel is instrumented for the respective cache events.

\todo{Add figures for each level of cache and a brief write up.}

\begin{table*}[t]
\centering
\begin{threeparttable}
    \centering
    \caption{OpenDwarf workload parameters $\Phi$}
    \begin{tabular}{l|c|c|c|c}
        \bf Benchmark & \bf tiny & small & medium & large\\\hline
        kmeans\textdagger& 256 & 2048 & 65536 & 524288
    \end{tabular}
    \begin{tablenotes}
    \item [\textdagger] increasing feature size the number of features.
    \item [*] Note the GPU devices ability to perform frequency scaling is
        unknown as no governor is provided.
    \item 
    \end{tablenotes}
    \label{tab:problem_sizes}
\end{threeparttable}
\end{table*}

For brevity, the figures to determine appropriate problem sizes for the remaining benchmarks have been omitted, and the final parameters used are presented in Table~\ref{tab:problem_sizes}.
The procedure to generate these remaining problem sizes is the same as those presented for k-means.

\begin{table*}[t]
\centering
\begin{threeparttable}
    \centering
    \caption{Program Arguments}
    \begin{tabular}{l|c}
        \bf Benchmark & \bf Arguments\\\hline
        kmeans & {\tt -g -p 26 -f} $\Phi$
    \end{tabular}
    \begin{tablenotes}
    \item no notes just yet
    \end{tablenotes}
    \label{tab:program_arguments}
\end{threeparttable}
\end{table*}

Where $\Phi$ is substituted as the argument for each benchmark, these are presented in Table~\ref{tab:program_arguments}.

Each {\bf Device} can be selected in a uniform way between applications using the same notation, on this system {\bf Device} comprises of {\tt -p 1 -d 0 -t 0} for the Intel Skylake CPU, where {\tt p} and {\tt d} are the integer identifier of the platform and device to respectively use, and {\tt -p 1 -d 0 -t 1} for the Nvidia Geforce GTX 1080 GPU.
Each application is run as {\bf Benchmark} {\bf Device} {\tt --} {\bf Arguments}.
For reproducibility the entire set of python scripts with all problem sizes is available in the Github repository. \todo{add url to git repo} 

\end{document}
